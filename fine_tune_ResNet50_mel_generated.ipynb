{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In /home/suzi/anaconda3/envs/tf2/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The text.latex.preview rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/suzi/anaconda3/envs/tf2/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The mathtext.fallback_to_cm rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/suzi/anaconda3/envs/tf2/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: Support for setting the 'mathtext.fallback_to_cm' rcParam is deprecated since 3.3 and will be removed two minor releases later; use 'mathtext.fallback : 'cm' instead.\n",
      "In /home/suzi/anaconda3/envs/tf2/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The validate_bool_maybe_none function was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/suzi/anaconda3/envs/tf2/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The savefig.jpeg_quality rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/suzi/anaconda3/envs/tf2/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The keymap.all_axes rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/suzi/anaconda3/envs/tf2/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The animation.avconv_path rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/suzi/anaconda3/envs/tf2/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The animation.avconv_args rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "print(tf.__version__)\n",
    "print(tf.test.is_gpu_available())\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "929"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mel-spectrogram parameters\n",
    "SR = 12000\n",
    "N_FFT = 512\n",
    "N_MELS = 96\n",
    "HOP_LEN = 256\n",
    "n_sample_fit = 60672\n",
    "N_FRAME = 238\n",
    "\n",
    "data_path = '/media/suzi/Seagate Expansion Drive/cough_task_for_job/data/audio/audio'\n",
    "train_path = data_path + '/train/*/*'\n",
    "test_path = data_path + '/test/*/*'\n",
    "val_path = data_path + '/validation/*/*'\n",
    "\n",
    "class_names = ['sick', 'not_sick']\n",
    "batch_size = 4\n",
    "train_samples = 1434 + 2282\n",
    "steps_per_epoch = train_samples // batch_size\n",
    "steps_per_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_str_tensor(tensor):\n",
    "    return bytes.decode(tensor.numpy())\n",
    "\n",
    "def stats(t):\n",
    "    s = 'mean:'\n",
    "    s += str(t.mean())\n",
    "    s += ' max:'\n",
    "    s += str(t.max())\n",
    "    s += ' min:'\n",
    "    s += str(t.min())\n",
    "    s += ' median:'\n",
    "    s += str(np.median(t))\n",
    "    s += ' std:'\n",
    "    s += str(t.std())\n",
    "    s += ' shape:'\n",
    "    s += str(t.shape)\n",
    "    s += ' dtype:'\n",
    "    s += str(t.dtype)\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_melgram(audio_path):\n",
    "    ''' Compute a mel-spectrogram and returns it in a shape of (1,1,96,1366), where\n",
    "    96 == #mel-bins and 1366 == #time frame\n",
    "    parameters\n",
    "    ----------\n",
    "    audio_path: path for the audio file.\n",
    "                Any format supported by audioread will work.\n",
    "    More info: http://librosa.github.io/librosa/generated/librosa.core.load.html#librosa.core.load\n",
    "    '''\n",
    "\n",
    "    src, sr = librosa.load(audio_path, sr=SR)  # whole signal\n",
    "    n_sample = len(src)\n",
    "    if n_sample < n_sample_fit:  # if too short\n",
    "        src = np.hstack((src, np.zeros(n_sample_fit - n_sample, dtype=np.float32)))\n",
    "    elif n_sample > n_sample_fit:  # if too long\n",
    "        src = src[(n_sample-n_sample_fit)/2:(n_sample+n_sample_fit)/2]\n",
    "#     stats(src)\n",
    "\n",
    "\n",
    "    spec = librosa.feature.melspectrogram(y=src, sr=SR, hop_length=HOP_LEN, n_fft=N_FFT, n_mels=N_MELS)\n",
    "#     stats(spec)\n",
    "\n",
    "    ret = librosa.amplitude_to_db(spec)\n",
    "    ret = (ret-ret.min())/(ret.max()-ret.min() + 1e-5)\n",
    "\n",
    "    ret = ret[..., np.newaxis]\n",
    "    return ret\n",
    "\n",
    "def plot_mel(ret):\n",
    "    plt.figure()\n",
    "    plt.pcolormesh(ret)\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'librosa' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-8c15b54e7eca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_melgram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/media/suzi/Seagate Expansion Drive/cough_task_for_job/data/audio/audio/test/sick/audioset_0-8Ht8OvuII_30_35.wav'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplot_mel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-0ff898c36cbc>\u001b[0m in \u001b[0;36mcompute_melgram\u001b[0;34m(audio_path)\u001b[0m\n\u001b[1;32m      9\u001b[0m     '''\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSR\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# whole signal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mn_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn_sample\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mn_sample_fit\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# if too short\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'librosa' is not defined"
     ]
    }
   ],
   "source": [
    "ret = compute_melgram('/media/suzi/Seagate Expansion Drive/cough_task_for_job/data/audio/audio/test/sick/audioset_0-8Ht8OvuII_30_35.wav')\n",
    "stats(ret)\n",
    "plot_mel(ret[..., 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_label(file_path):\n",
    "    # convert the path to a list of path components\n",
    "    parts = tf.strings.split(file_path, os.path.sep)\n",
    "    # The second to last is the class-directory\n",
    "    one_hot = parts[-2] == class_names\n",
    "    one_hot = tf.cast(one_hot, tf.int64)\n",
    "    # Integer encode the label\n",
    "    return tf.argmax(one_hot)\n",
    "# get_label('/media/suzi/Seagate Expansion Drive/cough_task_for_job/data/continuous_wavelet_transform/continuous_wavelet_transform/train/sick/audioset_0CroKP1sYVw_10_15.jpg')\n",
    "\n",
    "def process_path(file_path):\n",
    "    label = get_label(file_path)\n",
    "    # load the raw data from the file as a string\n",
    "    img = compute_melgram(bytes.decode(file_path.numpy()))\n",
    "    return img, label\n",
    "# img, label = process_path('/media/suzi/Seagate Expansion Drive/cough_task_for_job/data/audio/audio/test/sick/audioset_aiojV7s3hv0_55_60.wav')\n",
    "# stats(img)\n",
    "# label\n",
    "\n",
    "def set_dataset_shape(mel, label):\n",
    "    mel = tf.reshape(mel, [-1, N_MELS, N_FRAME, 1])\n",
    "    label = tf.reshape(label, (-1, ))\n",
    "    return mel, label\n",
    "\n",
    "def get_dataset(path, name):\n",
    "    ds = tf.data.Dataset.list_files(path)\n",
    "    ds = ds.map(lambda x: tf.py_function(process_path, [x], [tf.float32, tf.int64]), num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    ds = ds.cache(name).shuffle(buffer_size=100).batch(batch_size).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "    ds = ds.map(set_dataset_shape)\n",
    "    return ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = get_dataset(train_path, 'audio_train_catch')\n",
    "test_ds = get_dataset(test_path, 'audio_test_catch')\n",
    "val_ds = get_dataset(val_path, 'audio_val_catch')\n",
    "print(test_ds)\n",
    "for img, label in val_ds.take(1):\n",
    "    stats(img.numpy())\n",
    "    print(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "IMG_SHAPE = (N_MELS, N_FRAME, 3)\n",
    "# base_model = tf.keras.applications.DenseNet121(include_top=False, \n",
    "#                                                weights='imagenet', \n",
    "#                                                input_shape=IMG_SHAPE\n",
    "# )\n",
    "base_model = tf.keras.applications.ResNet50(include_top=False, \n",
    "                                            weights='imagenet', \n",
    "                                            input_shape=IMG_SHAPE,\n",
    "                                            classes=1)\n",
    "base_model.trainable = False\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(shape=(N_MELS, N_FRAME, 1))\n",
    "# x = tf.keras.applications.resnet.preprocess_input(inputs)\n",
    "x = tf.concat([inputs, inputs, inputs], axis=-1)\n",
    "\n",
    "if tf.random.uniform(()) > 0.5:\n",
    "    rand = tf.cast(tf.random.uniform(())*N_FRAME, tf.int32)\n",
    "    x1, x2 = tf.split(x, [rand, N_FRAME-rand], axis=2)\n",
    "    x = tf.concat([x2, x1], axis=2)\n",
    "    tf.print('split: ', rand)\n",
    "    \n",
    "x = base_model(x, training=False)\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "# x = layers.Dropout(0.5)(x)\n",
    "# x = layers.Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_learning_rate = 0.0001\n",
    "save_dir = './fine_tune_resnet50_mel_gemerated/'\n",
    "log_dir = save_dir + 'logs/'\n",
    "checkpoint_dir = save_dir + 'checkpoint_Feature_extraction/'\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
    "    base_learning_rate,\n",
    "    decay_steps=steps_per_epoch*5,\n",
    "    decay_rate=1,\n",
    "    staircase=False)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(lr_schedule)\n",
    "\n",
    "callbacks = [\n",
    "        tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3),\n",
    "        tf.keras.callbacks.TensorBoard(log_dir),\n",
    "        tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_dir,\n",
    "                                            monitor='val_accuracy',\n",
    "                                             verbose=1,\n",
    "                                             save_best_only=True,\n",
    "                                            save_weights_only=True)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "initial_epochs = 100\n",
    "\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_ds,\n",
    "                    epochs=initial_epochs,\n",
    "                    validation_data=val_ds,\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(checkpoint_dir)\n",
    "model.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable = True\n",
    "print(\"Number of layers in the base model: \", len(base_model.layers))\n",
    "\n",
    "# # Fine-tune from this layer onwards\n",
    "# fine_tune_at = 100\n",
    "\n",
    "# # Freeze all the layers before the `fine_tune_at` layer\n",
    "# for layer in base_model.layers[:fine_tune_at]:\n",
    "#     layer.trainable =  False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fine_tune_epochs = 100 + initial_epochs\n",
    "\n",
    "checkpoint_dir = save_dir + 'checkpoint_fine-tuning/'\n",
    "callbacks = [\n",
    "        tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5),\n",
    "        tf.keras.callbacks.TensorBoard(log_dir),\n",
    "        tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_dir,\n",
    "                                            monitor='val_accuracy',\n",
    "                                             verbose=1,\n",
    "                                             save_best_only=True,\n",
    "                                            save_weights_only=True)\n",
    "    ]\n",
    "lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
    "    base_learning_rate/3,\n",
    "    decay_steps=steps_per_epoch*5,\n",
    "    decay_rate=1,\n",
    "    staircase=False)\n",
    "optimizer = tf.keras.optimizers.Adam(lr_schedule)\n",
    "\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer = optimizer,\n",
    "              metrics=['accuracy'])\n",
    "model.summary()\n",
    "history_fine = model.fit(train_ds,\n",
    "                         epochs=fine_tune_epochs,\n",
    "                         initial_epoch=history.epoch[-1],\n",
    "                         validation_data=val_ds,\n",
    "                         callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.load_weights(checkpoint_dir)\n",
    "model.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
